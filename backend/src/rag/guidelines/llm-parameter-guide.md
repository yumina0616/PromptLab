# LLM 파라미터 설정 가이드

출처: “LLM Prompt Design Parameter Tips” (원문 요약)

LLM API로 프롬프트를 테스트할 때는 다양한 파라미터를 조절해 원하는 응답 품질과 톤을 만들어 낼 수 있습니다. 아래는 가장 많이 사용되는 설정과 활용 팁입니다.

---

## Temperature
- 낮을수록 결정론적: 가장 확률이 높은 다음 토큰을 선택.
- 높일수록 무작위성 증가 → 창의적/다양한 표현 유리.
- 사실 기반 QA나 요약: 낮은 온도 추천.
- 시/창작 등 자유로운 표현: 높은 온도 추천.

## Top P (Nucleus Sampling)
- 상위 확률 질량(top_p) 내 토큰만 고려.
- 낮으면 모델이 가장 확실한 답에 집중 → 사실성↑.
- 높이면 덜 확실한 단어도 포함 → 다양성↑.
- 일반적으로 Temperature와 Top P는 동시에 크게 조정하지 말고 한쪽만 조절하는 것이 권장됨.

## Max Length
- 모델이 생성할 최대 토큰 수.
- 너무 길면 비용과 잡담 증가, 짧으면 답이 잘리므로 상황에 맞춰 조절.

## Stop Sequences
- 특정 문자열이 생성되면 출력을 멈추도록 지정.
- 예: 최대 10개 리스트가 필요하면 “11”을 stop 시퀀스로 넣어 11번째 항목 생성을 차단.

## Frequency Penalty
- 응답과 프롬프트에 이미 나온 단어가 다시 등장할수록 패널티를 주는 설정.
- 값이 높을수록 반복이 줄어듦.
- Presence penalty와 동시에 크게 조절하기보다 둘 중 하나만 손보는 것이 일반적.

## Presence Penalty
- 한 번이라도 등장한 단어를 동일하게 패널티 처리.
- 등장 횟수(2회 vs 10회)에 상관없이 동일한 부담을 줌.
- 다양한 표현을 원하면 높이고, 특정 용어 유지가 필요하면 낮춤.

---

### 기본 권장
- Temperature 또는 Top P 중 하나만 조절.
- Frequency 또는 Presence penalty도 둘 중 하나만 주로 변경.
- 모델 버전별로 동작이 다르므로 실험을 통해 최적값을 찾는 것이 중요.

이 문서를 RAG 지식으로 추가해 두면 Tips 기능에서 “적절한 파라미터 값”까지 함께 제안할 수 있습니다.
